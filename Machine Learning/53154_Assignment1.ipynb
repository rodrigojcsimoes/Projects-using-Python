{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2487-2223 Machine Learning Assignment 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (25 points) - Zestimate this House\n",
    "\n",
    "Purchasing a house is a very big decision for most of us. Companies such as Zillows collected tons of data regarding the listing and sold price of American houses and build the predictive model, named *Zestimate*. You are expected to build a model similar as Zestimate to predict house price in Boston. \n",
    "\n",
    "![zestimate](https://i0.wp.com/www.housesoldeasy.com/wp-content/uploads/Screen-Shot-2016-08-15-at-7.22.09-PM.png?resize=300%2C258&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, LogisticRegression, LogisticRegressionCV, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, classification_report, precision_score, recall_score, confusion_matrix, precision_recall_curve\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder, KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded correctly.\n",
      "Features: X. Target variable (price): y.\n",
      "X shape:  (506, 13) y shape:  (506,)\n"
     ]
    }
   ],
   "source": [
    "### DON'T MODIFY - LOAD DATA ### \n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\" \n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]]) # FEATURES \n",
    "y = raw_df.values[1::2, 2] # TARGET VARIABLE\n",
    "assert X.shape[0] == y.shape[0], 'Mismatch in number of examples.'\n",
    "print('Data loaded correctly.')\n",
    "print('Features: X. Target variable (price): y.')\n",
    "print('X shape: ',X.shape, 'y shape: ', y.shape)\n",
    "### END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "        4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "        9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "        4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        7.8800e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.1 (5 points) \n",
    "Create train and test set, each contains 80% and 20% of the dataset, respectively, using *train_test_split* function in scikit-learn. Train a linear model on the train set and evaluate on the test set, report the training error and test error, respectively (as mean squared error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 21.64\n",
      "Test MSE: 24.29\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear model on the train set\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the train and test sets\n",
    "train_pred = model.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, train_pred)\n",
    "test_pred = model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_pred)\n",
    "\n",
    "# Print the training and test errors\n",
    "print('Training MSE: ' + str(round(train_mse, 2)))\n",
    "print('Test MSE: ' + str(round((test_mse), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "- Test error > training error -> model is overfitting\n",
    "- Low errors -> linear model is performing well on this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2 (5 points)\n",
    "\n",
    "Perform a 10-fold cross-validation on the whole data set. Show the averaged mean sqaured error on both train and test set at each fold. Explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Train MSE: 22.7375901544866 Test MSE: 14.99585287658265\n",
      "Fold 2 Train MSE: 20.85282114086763 Test MSE: 32.80452455251558\n",
      "Fold 3 Train MSE: 22.523708930639955 Test MSE: 17.599677176293966\n",
      "Fold 4 Train MSE: 21.77705689266885 Test MSE: 23.36882696420213\n",
      "Fold 5 Train MSE: 20.47610322792512 Test MSE: 35.1525526337427\n",
      "Fold 6 Train MSE: 22.262543646395933 Test MSE: 19.155888640230433\n",
      "Fold 7 Train MSE: 21.696285739040018 Test MSE: 24.140705056979968\n",
      "Fold 8 Train MSE: 22.206382674389694 Test MSE: 19.5477124442254\n",
      "Fold 9 Train MSE: 22.18801366254756 Test MSE: 20.26815402662742\n",
      "Fold 10 Train MSE: 21.465363892478834 Test MSE: 26.60813570390883\n",
      "Average Train MSE: 21.818586996144017\n",
      "Average Test MSE: 23.36420300753091\n"
     ]
    }
   ],
   "source": [
    "# Define 10-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Train a linear model and perform 10-fold cross-validation\n",
    "model = LinearRegression()\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for train_idx, test_idx in kfold.split(X, y):\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "    model.fit(X_train, y_train)\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    train_scores.append(mean_squared_error(y_train, train_pred))\n",
    "    test_scores.append(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "# Calculate and print the average mean squared error on both train and test set at each fold\n",
    "for i in range(10):\n",
    "    print('Fold', i+1, 'Train MSE:', train_scores[i], 'Test MSE:', test_scores[i])\n",
    "print('Average Train MSE:', np.mean(train_scores))\n",
    "print('Average Test MSE:', np.mean(test_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "We can see that the mean squared error on both the train and test set varies across the 10 folds of the cross-validation. The train MSE ranges from 20.48 to 22.74, while the test MSE ranges from 15 to 34.15. The average train MSE across the 10 folds is 21.82, which is similar to the training error we obtained in the previous question. The average test MSE across the 10 folds is 23.36, which is also similar to the test error we obtained in the previous question. This indicates that the linear model is still overfitting the training set to some extent, but the cross-validation results suggest that the overfitting is not as severe as we initially thought. Overall, the results suggest that the linear model has moderate predictive power on the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train MSE: 15.0, Test MSE: 15.0\n",
      "Fold 2: Train MSE: 32.8, Test MSE: 32.8\n",
      "Fold 3: Train MSE: 17.6, Test MSE: 17.6\n",
      "Fold 4: Train MSE: 23.37, Test MSE: 23.37\n",
      "Fold 5: Train MSE: 35.15, Test MSE: 35.15\n",
      "Fold 6: Train MSE: 19.16, Test MSE: 19.16\n",
      "Fold 7: Train MSE: 24.14, Test MSE: 24.14\n",
      "Fold 8: Train MSE: 19.55, Test MSE: 19.55\n",
      "Fold 9: Train MSE: 20.27, Test MSE: 20.27\n",
      "Fold 10: Train MSE: 26.61, Test MSE: 26.61\n",
      "Average Train MSE: 23.36\n",
      "Average Test MSE: 23.36\n"
     ]
    }
   ],
   "source": [
    "# Testar cross_validate\n",
    "# Define the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Define the cross-validation method\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "train_scores = -cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv)\n",
    "test_scores = -cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv)\n",
    "\n",
    "# Print the train and test scores for each fold\n",
    "for i in range(10):\n",
    "    print('Fold ' + str(i+1) + ': Train MSE: ' + str(round(train_scores[i], 2)) + ', Test MSE: ' + str(round(test_scores[i], 2)))\n",
    "\n",
    "# Print the averaged train and test scores over all folds\n",
    "print('Average Train MSE: ' + str(round(train_scores.mean(), 2)))\n",
    "print('Average Test MSE: ' + str(round(test_scores.mean(), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.3 (5 points) \n",
    " \n",
    "Add 2-degree squared polynomial features (with no interactions) and perform 10-fold cross-validation on the whole data set. Show the mean sqaured error on both train and test set at each fold. Explain your findings.\n",
    "\n",
    "Hint: you may use sklearn.preprocessing.PolynomialFeatures and check how it produces the polynomial features with/without interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Train MSE: 5.945580064091453 Test MSE: 8.756633514670874\n",
      "Fold 2 Train MSE: 5.165618833342352 Test MSE: 20.450915594154523\n",
      "Fold 3 Train MSE: 5.848722041597204 Test MSE: 13.296157702197098\n",
      "Fold 4 Train MSE: 5.678288605419227 Test MSE: 15.529903996120405\n",
      "Fold 5 Train MSE: 5.574123289586335 Test MSE: 13.882716128932193\n",
      "Fold 6 Train MSE: 6.020009539191569 Test MSE: 21.66779982244787\n",
      "Fold 7 Train MSE: 5.860454243811597 Test MSE: 9.74468351182666\n",
      "Fold 8 Train MSE: 6.0108147811452675 Test MSE: 7.218812707841904\n",
      "Fold 9 Train MSE: 6.0863294304804505 Test MSE: 7.326348286436629\n",
      "Fold 10 Train MSE: 5.7209094813251555 Test MSE: 12.867541906597461\n",
      "Average Train MSE: 5.791085030999062\n",
      "Average Test MSE: 13.074151317122562\n"
     ]
    }
   ],
   "source": [
    "# Add 2-degree squared polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Train a linear model with polynomial features and perform 10-fold cross-validation\n",
    "model = LinearRegression()\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for train_idx, test_idx in kfold.split(X_poly, y):\n",
    "    X_train, y_train = X_poly[train_idx], y[train_idx]\n",
    "    X_test, y_test = X_poly[test_idx], y[test_idx]\n",
    "    model.fit(X_train, y_train)\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    train_scores.append(mean_squared_error(y_train, train_pred))\n",
    "    test_scores.append(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "# Calculate and print the average mean squared error on both train and test set at each fold\n",
    "for i in range(10):\n",
    "    print('Fold', i+1, 'Train MSE:', train_scores[i], 'Test MSE:', test_scores[i])\n",
    "print('Average Train MSE:', np.mean(train_scores))\n",
    "print('Average Test MSE:', np.mean(test_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "We can see that adding 2-degree squared polynomial features to the dataset improves the performance of the linear model. The train MSE ranges from 5.17 to 6.09, while the test MSE ranges from 7.22 to 21.67. The average train MSE across the 10 folds is 5.72, which is lower than the training error we obtained in the previous question. The average test MSE across the 10 folds is 13.07, which is also lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.4 (10 points)\n",
    "\n",
    "Perform cross-validation using ridge regression and lasso regression on different feature combinations (linear features vs. polynomial features obtained earlier respectively. Explain which method works better in this case. Check the coefficients and explain the differences between ridge regression and lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Features with Ridge Regression\n",
      "Test MSE: 23.55453050994863\n",
      "Coef of Linear Features with Ridge Regression: [-1.04595278e-01  4.74432243e-02 -8.80467889e-03  2.55239322e+00\n",
      " -1.07770146e+01  3.85400020e+00 -5.41453810e-03 -1.37265353e+00\n",
      "  2.90141589e-01 -1.29116463e-02 -8.76074394e-01  9.67327945e-03\n",
      " -5.33343225e-01] \n",
      "\n",
      "Poly Features with Ridge Regression\n",
      "Test MSE: 14.430148406128884\n",
      "Coef of Poly Features with Ridge Regression: [-1.10562932e+00 -2.89393780e-01 -2.23752565e+00  9.12521424e-01\n",
      "  1.07129832e-01  4.72828819e+00  8.57778667e-01 -1.40989374e+00\n",
      "  2.04860314e+00 -4.61157073e-02 -1.64531921e+00  1.20050806e-01\n",
      " -1.13488290e-01  1.95916639e-03  1.36127122e-01  3.43338914e-01\n",
      "  3.00277719e+00 -8.99888402e-01  1.09657662e-01 -3.54182593e-03\n",
      " -6.58705406e-02  2.26736420e-01 -2.20769504e-02  1.95265866e-01\n",
      " -3.14998954e-04  1.82561742e-02  2.82404131e-05 -1.89439780e-03\n",
      " -5.01240048e-02 -1.50624424e-01  1.88717497e-02 -4.08798790e-04\n",
      " -7.80040639e-03 -4.68314381e-03  3.67675951e-04 -1.25052081e-03\n",
      "  6.34519551e-04 -3.89527648e-03  2.81674369e-02 -2.52890106e-01\n",
      "  1.46813150e+00  1.57064191e-01  2.57117015e-03  7.17782016e-02\n",
      "  4.13779058e-03  5.34381348e-04 -5.44023296e-02  1.09746843e-03\n",
      " -1.69408372e-02  9.12521424e-01 -1.05280579e+00 -4.45913415e+00\n",
      "  8.42647646e-02  1.97056770e+00 -6.28635634e-01 -4.35417351e-04\n",
      "  7.51604968e-01  3.35242499e-02 -4.65929786e-01 -1.53751408e-02\n",
      "  3.63951884e+00 -4.92829058e-01  3.55475463e+00 -1.27497724e+00\n",
      "  5.20317211e-03 -1.15583307e+00 -1.91428989e-02  1.12765142e+00\n",
      "  8.06990699e-01 -4.31559284e-02  7.21004330e-02 -2.25515075e-01\n",
      " -7.39627523e-03 -2.04878502e-01 -1.57197907e-03 -1.36121784e-01\n",
      "  3.19853393e-04  5.13701336e-04  1.35382115e-02 -3.92494566e-04\n",
      " -5.59280111e-03 -6.40366394e-04 -6.12667877e-03  2.86939339e-01\n",
      " -8.87789332e-02 -3.15986050e-03 -8.51189742e-02 -7.01346841e-03\n",
      "  7.68330400e-02 -1.38055347e-01  6.16246791e-03 -1.93066908e-02\n",
      "  6.03595950e-04 -3.28858414e-02 -2.18628569e-05  7.10036541e-03\n",
      " -8.78540664e-05 -4.13696753e-04  3.76355330e-02  1.36571511e-03\n",
      "  2.41896769e-02 -3.21882485e-05 -4.34677574e-04  1.55812803e-02] \n",
      "\n",
      "Linear Features with Lasso Regression\n",
      "Test MSE: 28.235449776422666\n",
      "Coef of Linear Features with Lasso Regression: [-0.06343729  0.04916467 -0.          0.         -0.          0.9498107\n",
      "  0.02090951 -0.66879     0.26420643 -0.01521159 -0.72296636  0.00824703\n",
      " -0.76111454] \n",
      "\n",
      "Poly Features with Lasso Regression\n",
      "Test MSE: 14.209048493719155\n",
      "Coef of Poly Features with Lasso Regression: [ 0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  1.26900566e-02 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  2.11539941e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -3.15970379e-03\n",
      " -0.00000000e+00 -1.74903523e-02  5.15438000e-04  0.00000000e+00\n",
      "  7.45486822e-05  2.00451437e-03  2.48375953e-04 -1.20683510e-04\n",
      "  0.00000000e+00  0.00000000e+00  3.84610941e-02  4.15318054e-04\n",
      " -0.00000000e+00 -5.49320078e-03  4.39297081e-04  3.44088578e-04\n",
      " -9.48035794e-04 -4.47180973e-03  1.42648511e-02 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.28938803e-03 -0.00000000e+00\n",
      " -6.05661673e-02  8.63426499e-04 -4.41202395e-02  6.73540947e-04\n",
      " -2.38470978e-02 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00  4.68969428e-03\n",
      "  0.00000000e+00 -4.15398226e-04 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -3.29482738e-02 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  2.03628590e-01  1.93441459e-02  0.00000000e+00 -8.17365203e-02\n",
      " -1.06242208e-02  0.00000000e+00  1.56128595e-02 -1.93361839e-02\n",
      " -4.02275295e-05  1.34629160e-03 -4.80324639e-05  1.69708619e-04\n",
      " -2.41212358e-03 -4.86702313e-04 -2.86778646e-03  7.12651141e-02\n",
      " -0.00000000e+00 -2.64514810e-03  0.00000000e+00 -3.01570198e-03\n",
      "  0.00000000e+00 -5.10228554e-03  2.08301529e-03  4.72651681e-02\n",
      " -8.17767817e-05 -9.71853579e-03 -3.21604945e-05  4.50877265e-03\n",
      " -2.32607272e-05 -1.43969183e-03 -3.52176204e-02 -1.39455644e-03\n",
      " -0.00000000e+00 -1.74848142e-05 -1.18582187e-04  3.19317651e-02]\n"
     ]
    }
   ],
   "source": [
    "# Define 10-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Add polynomial features to linear data\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Train a linear model with ridge regression and perform 10-fold cross-validation\n",
    "ridge = Ridge(alpha=1.0) #[0.01, 0.1, 1, 10, 100]\n",
    "ridge.fit(X, y)\n",
    "ridge_test_scores = -cross_val_score(ridge, X, y, cv=kfold, scoring='neg_mean_squared_error') # train = -ridge.score(X, y)\n",
    "print('Linear Features with Ridge Regression')\n",
    "print('Test MSE:', np.mean(ridge_test_scores))\n",
    "print('Coef of Linear Features with Ridge Regression:', ridge.coef_, '\\n')\n",
    "\n",
    "# Perform cross-validation on polynomial features using Ridge regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_poly, y)\n",
    "poly_ridge_test_scores = -cross_val_score(ridge, X_poly, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print('Poly Features with Ridge Regression')\n",
    "print('Test MSE:', np.mean(poly_ridge_test_scores))\n",
    "print('Coef of Poly Features with Ridge Regression:', ridge.coef_, '\\n')\n",
    "\n",
    "# Train a linear model with lasso regression and perform 10-fold cross-validation\n",
    "lasso = Lasso(alpha=1.0)\n",
    "lasso.fit(X, y)\n",
    "lasso_test_scores = -cross_val_score(lasso, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print('Linear Features with Lasso Regression')\n",
    "print('Test MSE:', np.mean(lasso_test_scores))\n",
    "print('Coef of Linear Features with Lasso Regression:', lasso.coef_, '\\n')\n",
    "\n",
    "# Perform cross-validation on polynomial features using Lasso regression\n",
    "lasso = Lasso(alpha=1.0)\n",
    "lasso.fit(X_poly, y)\n",
    "poly_lasso_test_scores = -cross_val_score(lasso, X_poly, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print('Poly Features with Lasso Regression')\n",
    "print('Test MSE:', np.mean(poly_lasso_test_scores))\n",
    "print('Coef of Poly Features with Lasso Regression:', lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Analysis**\n",
    "\n",
    "From the output, we can see that the polynomial features models perform much better when compared to the linear models. The polynomial feature model is has MSE of around 14, while the linear model is always above 23.\n",
    "\n",
    "The main difference between Ridge regression and Lasso regression is the type of regularization term they use. Ridge regression uses L2 regularization, which adds a penalty term to the sum of squares of the coefficients of the independent variables. Lasso regression, on the other hand, uses L1 regularization, which adds a penalty term to the sum of absolute values of the coefficients of the independent variables.\n",
    "\n",
    "One of the main consequences of this difference is that Ridge regression tends to produce solutions with small but non-zero coefficients for all independent variables, while Lasso regression tends to produce sparse solutions where some coefficients are exactly zero. This means that Lasso regression can be used for feature selection, as it effectively eliminates some independent variables from the model, while Ridge regression typically retains all independent variables in the model.\n",
    "\n",
    "In our case, the method with lower MSE is the polynomial feature Lasso regression (14.2), but only by a small margin when compared to the polynomial feature Ridge regression (14.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (25 points) - Cancer Detection\n",
    "\n",
    "Given a dataset with features that are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass, which describes characteristics of the cell nuclei present in the image, let's try to predict whether the patients are diagnosed as Malignant (M) or Benign (B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA \n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\"\"\"\n",
    "DOCS:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer\n",
    "\"\"\"\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1 (5 points) \n",
    "Use logistic regression to train the dataset through cross-validation, report the score on train and test set, respectively. Explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.954\n",
      "Test score: 0.956\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a logistic regression model with L2 regularization and higher number of iterations\n",
    "clf = LogisticRegression(penalty='l2', max_iter=3000, random_state=42)\n",
    "\n",
    "# Train the model using 5-fold cross-validation on the train set\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "\n",
    "# Report the mean score on the train set\n",
    "print(f\"Train score: {scores.mean():.3f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "clf.fit(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "\n",
    "# Report the score on the test set\n",
    "print(f\"Test score: {test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "The logistic regression model achieved a high score on both the training set and the test set. The difference in score between the training set and the test set is also very small. This indicates that the model is not overfitting and can generalize well to unseen data. Overall, these findings suggest that the logistic regression model is a good fit for the breast cancer dataset and can accurately predict whether a tumor is benign or malignant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2 (5 points) \n",
    "By default, sklearn's logistic regression uses the L2 regularization. Now use the logistic regression without any regularzation to perform cross validation, report what do you find on train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.923\n",
      "Test score: 0.965\n"
     ]
    }
   ],
   "source": [
    "# Initialize logistic regression without regularization\n",
    "lr = LogisticRegression(penalty='none', solver='sag', max_iter=5000)\n",
    "\n",
    "# Perform cross-validation with 5 folds\n",
    "train_scores = cross_val_score(lr, X, y, cv=5)\n",
    "\n",
    "# Fit the model on the train set\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance on the test set\n",
    "test_score = lr.score(X_test, y_test)\n",
    "\n",
    "# Report the average train score\n",
    "print(\"Train score: {:.3f}\".format(train_scores.mean()))\n",
    "\n",
    "# Report the test score\n",
    "print(\"Test score: {:.3f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.3 (15 points) \n",
    "Check how many Benign and Malignant cases in the dataset. What might be the problem if we use the default score of the logistic regression cross-validation? Now adjust the class weight of M and L and retrain the model again to bias toward Malignant, using the relative weight of M and L as 2:1. What about the relaive weight to be 5:1, or 10:1? Explain what you find.\n",
    "\n",
    "Hint: you can use LogisticRegressionCV to combine LogisticRegression and cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign count: 357\n",
      "Malignant count: 212\n"
     ]
    }
   ],
   "source": [
    "benign_count = sum(y == 1)\n",
    "malignant_count = sum(y == 0)\n",
    "\n",
    "print(f\"Benign count: {benign_count}\")\n",
    "print(f\"Malignant count: {malignant_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with using the default score of the logistic regression cross-validation is that it treats the two classes equally, which is not the case in this dataset. There are more benign cases than malignant cases. However, identifying malignant cases is the most important class to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weight: {0: 2, 1: 1}\n",
      "Train score: 0.914\n",
      "Test score: 0.947\n",
      "\n",
      "Class weight: {0: 5, 1: 1}\n",
      "Train score: 0.893\n",
      "Test score: 0.939\n",
      "\n",
      "Class weight: {0: 10, 1: 1}\n",
      "Train score: 0.854\n",
      "Test score: 0.877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = [{0: 2, 1: 1}, {0: 5, 1: 1}, {0: 10, 1: 1}]\n",
    "    \n",
    "for c in class_weights:\n",
    "    # Train the model with cross-validation and the class weight dictionary\n",
    "    model = LogisticRegressionCV(class_weight=c, solver='sag', max_iter=5000)\n",
    "\n",
    "    # Perform cross-validation with 5 folds\n",
    "    train_scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "    # Fit the model on the train set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the performance on the test set\n",
    "    test_score = model.score(X_test, y_test)\n",
    "\n",
    "    print(f\"Class weight: {c}\")\n",
    "    # Report the average train score\n",
    "    print(\"Train score: {:.3f}\".format(train_scores.mean()))\n",
    "\n",
    "    # Report the test score\n",
    "    print(\"Test score: {:.3f}\\n\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "Overall, the first configuration with a class weight of {0: 2, 1: 1} performs the best with the highest accuracy scores on both the train and test sets. This configuration strikes a balance between the two classes, with a slightly higher weight placed on malignant cance while still allowing the model to perform well on both classes. Therefore, this weight configuration is the best for this classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (50 points) - Call Me Maybe? \n",
    "\n",
    "\n",
    "\n",
    "![telemarketing](https://neilpatel.com/wp-content/uploads/2019/08/profissional-de-telemarketing-sorridente.jpeg)\n",
    "\n",
    "Telemarketing is a method of direct marketing in which a salesperson solicits prospective customers to buy products or services over the phone. It has become one of the most widely used marketing campaign methods to engage with customers with product and service opportunity. We have collected real data from a Portuguese retail bank, from May 2008 to June 2013 with thousands of phone contacts. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The current practice of many data teams is to build such propensity models and predict customer's probability to adopt the product and target them from the highest probability to the lowest probability. Note that telemarketing may incur some costs for contacting the customer, thus the success (i.e., the generated profit) of using machine learning model requries further inspection.  As the data scientist, you are asked to build a propensity model to evaluate the effectiveness of their telemarketing campaigns, i.e. whether the customer subscribed to the term deposit.  \n",
    "\n",
    "**Telemarketing Dataset (bank.csv)**\n",
    "All customers are contained in the file bank.csv. Each line of this file after the header row represents one customer of the Portuguese bank, and has the following format:\n",
    "\n",
    "### bank client data:\n",
    "- age (numeric)\n",
    "- job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "- marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "- education (categorical: 'primary', 'secondary', 'tertiary')\n",
    "- balance: amcount of bank account balance\n",
    "- default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "- housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "- loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "\n",
    "### related with the last contact of the current campaign:\n",
    "- contact: contact communication type (categorical: 'cellular','telephone', 'unknown')\n",
    "- day: last contact day of month\n",
    "- month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "- duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. \n",
    "\n",
    "### other attributes:\n",
    "- campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "- pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; -1 means client was not previously contacted)\n",
    "- previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "- poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "- y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "\n",
    "\n",
    "Answer the following questions using the provided dataset. You can write down intermediate results towards the final answers. If any model invovles random_state, set it to be 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1787</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>19</td>\n",
       "      <td>oct</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>4789</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>11</td>\n",
       "      <td>may</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>339</td>\n",
       "      <td>4</td>\n",
       "      <td>failure</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1350</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>16</td>\n",
       "      <td>apr</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>330</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1476</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>3</td>\n",
       "      <td>jun</td>\n",
       "      <td>199</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          job  marital  education default  balance housing loan  \\\n",
       "0   30   unemployed  married    primary      no     1787      no   no   \n",
       "1   33     services  married  secondary      no     4789     yes  yes   \n",
       "2   35   management   single   tertiary      no     1350     yes   no   \n",
       "3   30   management  married   tertiary      no     1476     yes  yes   \n",
       "4   59  blue-collar  married  secondary      no        0     yes   no   \n",
       "\n",
       "    contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  cellular   19   oct        79         1     -1         0  unknown  no  \n",
       "1  cellular   11   may       220         1    339         4  failure  no  \n",
       "2  cellular   16   apr       185         1    330         1  failure  no  \n",
       "3   unknown    3   jun       199         4     -1         0  unknown  no  \n",
       "4   unknown    5   may       226         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank = pd.read_csv('bank.csv', sep=';')\n",
    "bank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           int64\n",
       "job          object\n",
       "marital      object\n",
       "education    object\n",
       "default      object\n",
       "balance       int64\n",
       "housing      object\n",
       "loan         object\n",
       "contact      object\n",
       "day           int64\n",
       "month        object\n",
       "duration      int64\n",
       "campaign      int64\n",
       "pdays         int64\n",
       "previous      int64\n",
       "poutcome     object\n",
       "y            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>day</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>41.170095</td>\n",
       "      <td>1422.657819</td>\n",
       "      <td>15.915284</td>\n",
       "      <td>263.961292</td>\n",
       "      <td>2.793630</td>\n",
       "      <td>39.766645</td>\n",
       "      <td>0.542579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.576211</td>\n",
       "      <td>3009.638142</td>\n",
       "      <td>8.247667</td>\n",
       "      <td>259.856633</td>\n",
       "      <td>3.109807</td>\n",
       "      <td>100.121124</td>\n",
       "      <td>1.693562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>-3313.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>444.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>1480.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>87.000000</td>\n",
       "      <td>71188.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>3025.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>871.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age       balance          day     duration     campaign  \\\n",
       "count  4521.000000   4521.000000  4521.000000  4521.000000  4521.000000   \n",
       "mean     41.170095   1422.657819    15.915284   263.961292     2.793630   \n",
       "std      10.576211   3009.638142     8.247667   259.856633     3.109807   \n",
       "min      19.000000  -3313.000000     1.000000     4.000000     1.000000   \n",
       "25%      33.000000     69.000000     9.000000   104.000000     1.000000   \n",
       "50%      39.000000    444.000000    16.000000   185.000000     2.000000   \n",
       "75%      49.000000   1480.000000    21.000000   329.000000     3.000000   \n",
       "max      87.000000  71188.000000    31.000000  3025.000000    50.000000   \n",
       "\n",
       "             pdays     previous  \n",
       "count  4521.000000  4521.000000  \n",
       "mean     39.766645     0.542579  \n",
       "std     100.121124     1.693562  \n",
       "min      -1.000000     0.000000  \n",
       "25%      -1.000000     0.000000  \n",
       "50%      -1.000000     0.000000  \n",
       "75%      -1.000000     0.000000  \n",
       "max     871.000000    25.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     4000\n",
       "yes     521\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 (15 points)\n",
    "\n",
    "Split the data into 80% training set and 20% test set. **Build a pipeline to preprocess the indicated numerical features and categorical features separately**. For numerical features 'balance', 'campaign', standardize these features. For categorical features 'job', 'marital', 'education', 'default', transform them through one-hot encoding. For the numeric feature 'age', convert it into the quartile categorical variable and transform it through one-hot encoding. \n",
    "\n",
    "Train a Logistic regression model with L2 regularization using 5-fold cross validation (default hyperparameter) on the train set and show the accuracy, precision, recall on the train set. Explain whether the model is useful for the bank to identify the customer propensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bank.drop('y', axis=1)\n",
    "y = bank['y'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94      3193\n",
      "           1       0.00      0.00      0.00       423\n",
      "\n",
      "    accuracy                           0.88      3616\n",
      "   macro avg       0.44      0.50      0.47      3616\n",
      "weighted avg       0.78      0.88      0.83      3616\n",
      "\n",
      "Accuracy: 0.883\n",
      "Overall precision: 0.780\n",
      "Overall recall: 0.883\n"
     ]
    }
   ],
   "source": [
    "# Convert 'age' into quartile categorical variable\n",
    "X_train['age'] = pd.qcut(X_train['age'], 4, labels=[0, 1, 2, 3])\n",
    "X_test['age'] = pd.qcut(X_test['age'], 4, labels=[0, 1, 2, 3])\n",
    "\n",
    "# Define the column transformer for the numerical and categorical features\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_transformer, ['balance', 'campaign']),\n",
    "    ('cat', cat_transformer, ['job', 'marital', 'education', 'default', 'age'])\n",
    "])\n",
    "\n",
    "# Define the logistic regression model\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegressionCV(penalty='l2', cv=5))\n",
    "])\n",
    "\n",
    "# Fit the model on the training set\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set and calculate accuracy, precision, and recall\n",
    "y_pred = clf.predict(X_train)\n",
    "accuracy = clf.score(X_train, y_train)\n",
    "report = classification_report(y_train, y_pred, zero_division=0)\n",
    "precision = precision_score(y_train, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_train, y_pred, average='weighted')\n",
    "\n",
    "print(report)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f'Overall precision: {precision:.3f}')\n",
    "print(f'Overall recall: {recall:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "Based on the classification report, the model has an overall accuracy of 0.883, which indicates that it is able to predict the customer propensity with reasonable accuracy. However, it is important to note that the precision and recall for the positive class ('yes') are 0, which suggests that the model have difficulties identifying customers who are likely to accept the offer.\n",
    "\n",
    "In addition, the dataset is imbalanced, with the negative class ('no') being much more prevalent than the positive class. This can lead to bias in the model's predictions and may result in an overly optimistic assessment of the model's performance.\n",
    "\n",
    "Calculating the overall precision and recall we get the values 0.78 and 0.88, respectively. We observe the recall and accuracy have the same value.\n",
    "\n",
    "Therefore, while the model shows promise, it may require further refinement and evaluation before it can be deemed useful for the bank to identify customer propensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2 (10 points)\n",
    "\n",
    "Now add more features to the model to see if we can improve the performance (categorical features: 'housing', 'loan' and numerical features: 'day', 'duration'). Use the preprocess pipeline built previously to transform the data. Train a Logistic regression model with L1 regularization using 5-fold cross validation on the train set, by fine-tuning the hyperparameter alpha, i.e. the regularization strength from [0.001, 0.01, 0.1, 1]. Choose the correct score function that reflect the current data team's practice. Report the average score with the best hyperparameter. Does model performance improve, and if so, how?\n",
    "\n",
    "Expalin whether all features are useful for making prediction and why. List top 5 features that contribute to the prediction the most. If not all features are useful, list those unuseful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter: 1\n",
      "Best average score: 0.44540270977130164\n",
      "Top 5 features:\n",
      "              feature      coef  abs_coef\n",
      "3           duration  1.355564  1.355564\n",
      "32          loan_yes -1.001370  1.001370\n",
      "6   job_entrepreneur -0.858314  0.858314\n",
      "12       job_student  0.753710  0.753710\n",
      "30       housing_yes -0.657190  0.657190\n",
      "\n",
      "Unuseful features:\n",
      "                 feature  coef  abs_coef\n",
      "31              loan_no   0.0       0.0\n",
      "18       marital_single   0.0       0.0\n",
      "20  education_secondary   0.0       0.0\n",
      "26                age_1   0.0       0.0\n",
      "24          default_yes   0.0       0.0\n"
     ]
    }
   ],
   "source": [
    "# Features list\n",
    "num_features = ['balance', 'campaign', 'day', 'duration']\n",
    "cat_features = ['job', 'marital', 'education', 'default', 'age', 'housing', 'loan']\n",
    "\n",
    "# Update the preprocessor with the new feature lists\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features),\n",
    "        ('cat', OneHotEncoder(), cat_features)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with L1 regularization\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_grid = {'classifier__C': [1 / 0.001, 1 / 0.01, 1 / 0.1, 1]}\n",
    "\n",
    "# Perform a Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Report the best hyperparameter and average score\n",
    "best_hyperparameter = grid_search.best_params_['classifier__C']\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best hyperparameter:\", best_hyperparameter)\n",
    "print(\"Best average score:\", best_score)\n",
    "\n",
    "\n",
    "# Train the model with the best hyperparameter\n",
    "best_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(penalty='l1', solver='liblinear', C=best_hyperparameter, class_weight='balanced', random_state=42))\n",
    "])\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "coef = best_pipeline.named_steps['classifier'].coef_[0]\n",
    "onehot_columns = best_pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names(cat_features)\n",
    "\n",
    "# Combine feature names and coefficients\n",
    "features = np.array(num_features + list(onehot_columns))\n",
    "feature_importances = pd.DataFrame({'feature': features, 'coef': coef})\n",
    "\n",
    "# Sort features by their absolute coefficient values\n",
    "feature_importances['abs_coef'] = np.abs(feature_importances['coef'])\n",
    "feature_importances = feature_importances.sort_values(by='abs_coef', ascending=False)\n",
    "\n",
    "# Display the top 5 features\n",
    "print(\"Top 5 features:\\n\", feature_importances.head(5))\n",
    "\n",
    "# Display unuseful features (with a coefficient close to zero)\n",
    "unuseful_features = feature_importances.sort_values(by='abs_coef', ascending=True) \n",
    "print(\"\\nUnuseful features:\\n\", unuseful_features.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "The best hyperparameter (C) found using Grid Search is 1, and the best average F1 score is 0.4458. \n",
    "\n",
    "Not all features are useful for making predictions. The output shows that some features have a coefficient of zero, indicating that they do not contribute to the model's predictions. These features are considered unuseful for making predictions. Specifically, the features \"loan_no\", \"marital_single\", \"education_secondary\", \"age_1\", and \"default_yes\" have coefficients of zero, indicating that they do not influence the model's predictions.\n",
    "\n",
    "On the other hand, the top 5 features with the highest absolute coefficients are \"duration\", \"loan_yes\", \"job_entrepreneur\", \"job_student\", and \"housing_yes\". These features are considered the most useful for making predictions as they have the most significant impact on the model's outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3 (10 points)\n",
    "\n",
    "Now use the best model found in the cross-validation to predict the test set, show the obtained confusion matrix. Assume that targeting each customer would cost 10 euros and if the customer subscribe, the company would earn 50 euros. If we perform targeted telemarketing to all customers that are predicted to subscribe in the test set, what's the resulting profit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[650 157]\n",
      " [ 30  68]]\n",
      "Total Profit: 1150\n"
     ]
    }
   ],
   "source": [
    "# Predict the test set\n",
    "y_pred_test = best_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Get values from the confusion matrix\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "\n",
    "# Cost and profit settings\n",
    "cost_per_customer = 10\n",
    "profit_per_customer = 50\n",
    "\n",
    "# Calculate profit\n",
    "total_profit = (TP * profit_per_customer) - (TP + FP) * cost_per_customer\n",
    "print(\"Total Profit:\", total_profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.4 (10 points)\n",
    "\n",
    "Now adjust the decision threshold in order to optimize the obtained profit. What would be the resulting threshold and profit? Is the propensity model built based on the targeting predicted probability useful in terms of profit maximizing? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold: 0.6353875394659373\n",
      "Optimal Profit: 1300\n"
     ]
    }
   ],
   "source": [
    "# Profit function\n",
    "def calculate_profit(TP, FP):\n",
    "    return (TP * profit_per_customer) - (TP + FP) * cost_per_customer\n",
    "\n",
    "# Get the predicted probabilities of the positive class\n",
    "y_pred_proba = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision, recall, and thresholds using precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "thresholds = np.append(thresholds, 1)\n",
    "\n",
    "# Calculate true positives and false positives for each threshold\n",
    "true_positives = (recall * y_test.sum()).astype(int)\n",
    "false_positives = ((1 - precision) * true_positives / precision).astype(int)\n",
    "\n",
    "# Calculate the profit for each threshold\n",
    "profits = [calculate_profit(tp, fp) for tp, fp in zip(true_positives, false_positives)]\n",
    "\n",
    "# Find the optimal threshold and profit\n",
    "optimal_index = np.argmax(profits)\n",
    "optimal_threshold = thresholds[optimal_index]\n",
    "optimal_profit = profits[optimal_index]\n",
    "\n",
    "print(\"Optimal Threshold:\", optimal_threshold)\n",
    "print(\"Optimal Profit:\", optimal_profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "The code uses the best model from cross-validation to predict the probability of a customer subscribing. Precision, recall, and corresponding thresholds are then computed using the precision_recall_curve function. The goal is to find the optimal decision threshold that maximizes profit, achieved by calculating true positives and false positives for each threshold and computing the profit for each threshold using the calculate_profit function.\n",
    "\n",
    "The output shows that the propensity model, based on the targeting predicted probability, is useful for maximizing profit. By adjusting the decision threshold, the company can optimize their telemarketing campaign to target customers more likely to subscribe and increase profit. The original threshold yielded a profit of 1,150 euros, but optimizing the threshold increased profit to 1,300 euros, demonstrating the usefulness of the propensity model in maximizing profit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.5 (5 points)\n",
    "\n",
    "Now train a random forest model, with 10 decision trees and max_depth=2, what is the profit that can be achieved given the threshold that you identified earlier? Do you need to increase or decrese the threshold to maximize the profit using random forest model? Explain your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profit with Random Forest: 0\n",
      "Optimal Threshold for Random Forest: 0.18084288357003442\n",
      "Maximized Profit with Random Forest: 580\n"
     ]
    }
   ],
   "source": [
    "# Train the random forest model\n",
    "rf_pipeline = RandomForestClassifier(n_estimators=10, max_depth=2, random_state=42)\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', rf_pipeline)\n",
    "])\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Calculate profit with the threshold identified earlier\n",
    "y_pred_proba_rf = rf_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_pred_rf = (y_pred_proba_rf > optimal_threshold).astype(int)\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "TN, FP, FN, TP = conf_matrix_rf.ravel()\n",
    "profit_rf = (TP * profit_per_customer) - (TP + FP) * cost_per_customer\n",
    "\n",
    "print(\"Profit with Random Forest:\", profit_rf)\n",
    "\n",
    "# Optimize the threshold for the random forest model\n",
    "profits_rf = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold_rf = (y_pred_proba_rf > threshold).astype(int)\n",
    "    conf_matrix_rf = confusion_matrix(y_test, y_pred_threshold_rf)\n",
    "\n",
    "    TN, FP, FN, TP = conf_matrix_rf.ravel()\n",
    "    profit_rf = (TP * profit_per_customer) - (TP + FP) * cost_per_customer\n",
    "    profits_rf.append(profit_rf)\n",
    "\n",
    "max_profit_index_rf = np.argmax(profits_rf)\n",
    "optimal_threshold_rf = thresholds[max_profit_index_rf]\n",
    "max_profit_rf = profits_rf[max_profit_index_rf]\n",
    "\n",
    "print(\"Optimal Threshold for Random Forest:\", optimal_threshold_rf)\n",
    "print(\"Maximized Profit with Random Forest:\", max_profit_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "According to the results, adjusting the threshold for the random forest model can maximize profits. The best threshold value for this model is 0.1812, which is lower than the value found for the logistic regression model. When using the logistic regression threshold for the random forest model, the profits are zero, indicating poor performance. Optimizing the threshold specifically for the random forest model increases profits to 580 euros.\n",
    "\n",
    "The difference in optimal thresholds for these models can be attributed to their unique ways of capturing patterns in the data and scaling prediction probabilities. Thus, a threshold that works well for one model may not be effective for the other.\n",
    "\n",
    "In conclusion, to maximize profits with the random forest model, the threshold should be lowered to 0.1812. Although the profits are lower than those achieved with the logistic regression model, they are still better than using the logistic regression threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
